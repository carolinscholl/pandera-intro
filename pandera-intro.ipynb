{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pandera as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Welcome to this mini-tutorial on the python library pandera. Pandera lets you validate dataframe objects against specified data schemas. This can be easily integrated into your data pipelines in order to continuously test your data, spot data errors or schema updates, and generally support data quality monitoring.\n",
    "\n",
    "Let us start by reading in a small dataframe. Its column names should already give you a hint on each intended data type."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"dummy_data\", \"dummy.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, to raise any doubts, here is the schema again:\n",
    "\n",
    "| Column/variable name | Data type                                     |\n",
    "|----------------------|-----------------------------------------------|\n",
    "| var_bool             | boolean                                       |\n",
    "| var_signed_int       | signed integer (positive and negative values) |\n",
    "| var_unsigned_int     | unsigned integer (only non-negative values)   |\n",
    "| var_float            | floating point number                         |\n",
    "| var_str              | string                                        |\n",
    "| var_date             | date (format YYYY-MM-DD)                      |\n",
    "| var_categorical      | categorical                                   |\n",
    "| var_constant         | constant                                      |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define pandera schema as DataFrameSchema object\n",
    "\n",
    "Let's try to write a schema for it! For this we have to create a pandera DataFrameSchema object:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "schema = pa.DataFrameSchema()\n",
    "print(type(schema))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we add a first column (a pandera Column object):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "schema = schema.add_columns({\"Var_bool\": pa.Column(bool)})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can already try to validate our dataframe against the specified schema. The validate() function throws an error if one or multiple schema errors are encountered. Otherwise it just returns the validated dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "schema.validate(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ooops, the validate function tells us that the specified column in the schema \"Var_bool\" is no in the dataframe. That's because the column in the dataframe is written in lowercase \"var_bool\". Let us try that again then. We try to rename the column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "# TODO: correct the name of the column in the schema. The rename_columns function expects a dictionary with old names as keys and new names as values (replace the dots with the dictionary)\n",
    "schema = schema.rename_columns(...)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the updated schema and try validation again."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "schema.validate(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataframe was returned and survived our first validation. Given we specified the column var_bool with a correct type, the validate function does not throw an error. However, what if we want it to throw an error because there are variables in the dataframe which are not mentioned in the schema (yet)? Let us look at the schema."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apart from the columns, there are a couple other properties of interest here.\n",
    "\n",
    "**checks**: Here you can add user-defined checks for columns or groups of columns. Checks can also be directly given as an argument to a column object, but are limited to that column then. We will look into that later.\n",
    "\n",
    "**coerce**: If true pandera tries to coerce/cast the column into the specified type. This is helpful if your columns in the dataframe are not specifically typed, but you just read it the frame from, e.g., a csv file. For instance, the inferred data type for a column of zeros and ones by pandas would probably be integer. However, if you specify its datatime to be bool in the schema and set coerce to True, validation should not throw an error because coercion should work.\n",
    "\n",
    "**strict**: If set to true means that if we want additional unexpected columns in the dataframe to throw a validation error, we have to set the attribute \"strict\" to True.\n",
    "\n",
    "**ordered**: If set to true, the columns in the dataframe must appear in the same order as the columns in the schema.\n",
    "\n",
    "If you want to know more about all configurable properties of a DataFrameSchema, have a look at the [docs](https://pandera.readthedocs.io/en/stable/reference/generated/pandera.api.pandas.container.DataFrameSchema.html#pandera.api.pandas.container.DataFrameSchema)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Update a schema property such that we will receive a validation error because there are additional columns in our original dataframe that are not (yet) in the schema.\n",
    "schema"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "schema.validate(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we should get a schema error. The column from the dataframe \"var_signed_int\" does not appear in our schema. Let us redefine our schema and add all columns. We deliberately define a wrong datatype for all columns but the var_bool column here because we want to learn about lazy validation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "schema = pa.DataFrameSchema({\"var_bool\": pa.Column(bool),\n",
    "                             \"var_signed_int\": pa.Column(bool),\n",
    "                             \"var_unsigned_int\": pa.Column(bool),\n",
    "                             \"var_float\": pa.Column(bool),\n",
    "                             \"var_str\": pa.Column(bool),\n",
    "                             \"var_date\": pa.Column(bool),\n",
    "                             \"var_categorical\": pa.Column(bool),\n",
    "                             \"var_constant\": pa.Column(bool)})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You might want to repeatedly execute validation to find out the errors. By default pandera will stop validation at the first encountered error. If you want to be informed about all errors during validation, you need to provide the lazy keyword to the validate function. This is useful when data validation is applied to an incrementing dataset in a pipeline. You want to be informed about all errors and not re-execute the pipeline again and again to detect all problems."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "schema.validate(df, lazy=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that there are 8 errors here. Let us try to fix them!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Update the schema below with the correct datatypes. Ignore the difference between unsigned and signed int for now\n",
    "schema = pa.DataFrameSchema({\"var_bool\": pa.Column(bool),\n",
    "                             \"var_signed_int\": pa.Column(...),\n",
    "                             \"var_unsigned_int\": pa.Column(...),\n",
    "                             \"var_float\": pa.Column(...),\n",
    "                             \"var_str\": pa.Column(...),\n",
    "                             \"var_date\": pa.Column(...),\n",
    "                             \"var_categorical\": pa.Column(...),\n",
    "                             \"var_constant\": pa.Column(...)})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Hints:**\n",
    "- Overall, [this list](https://pandera.readthedocs.io/en/stable/reference/dtypes.html) of data types might help, but for basic data types like integer you can also use pure Python built-in data types.\n",
    "- Check out the pandera datatypes [pa.Category](https://pandera.readthedocs.io/en/stable/reference/generated/pandera.dtypes.Category.html) and [pa.DateTime](https://pandera.readthedocs.io/en/stable/reference/generated/pandera.dtypes.DateTime.html).\n",
    "- If pandera complains about missing values in the dataframe during validation which are actually possible, you might want to consider changing the keyword \"nullable\" which by default is set to False.\n",
    "- Like explained above coercion sometimes helps to convince pandera of the datatype you mean. If you check df.dtypes you will notice that most columns have the pandas-inferred datatype \"object\". Pandera accepts string for basically everything, but if you want to define a more specific datatype, try to set the coerce argument to True. You can also set it to true for the whole schema. This will overwrite the column-specific specification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "schema.validate(df, lazy=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us differentiate between the data types for the signed and unsigned integer. A possibility to do this is by working with [pandera Checks](https://pandera.readthedocs.io/en/stable/reference/generated/pandera.api.checks.Check.html)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Screen the possible checks and choose an appropriate one. Update the schema accordingly:\n",
    "schema = schema.update_columns({\"var_unsigned_int\": {\"dtype\": int, \"checks\": [...]}})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us verify schema validation still runs through"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "schema.validate(df, lazy=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congrats! You wrote your first schema including a check."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schema inference\n",
    "\n",
    "Imagine you have a complex dataset and you do not want to write the schema from scratch. After all, time is limited and manual processes are error-prone. Luckily, pandera comes with a function that can generate a first draft of a pandera schema. Let's try out which data types pandera infers from our dummy csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inferred_schema = pa.infer_schema(df)\n",
    "print(inferred_schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen, the inference of bool, integer and floating point variables worked nicely. However, the data types of string variables, categorical variables and date variables have to be further specified as they are currently just typed \"objects\".\n",
    "\n",
    "⚠️ The results from the infer_schema function from pandera should always be seen as a first draft and should be updated using your domain knowledge.\n",
    "\n",
    "You can edit the schema directly (e.g. using the update_columns() function). What I prefer is to save the schema as a file and edit it there. Let's try that. Pandera allows you to save the schemas as yaml-file, json-file or python-script.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save schema as file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "inferred_schema.to_json(os.path.join(\"dummy_data\", \"inferred_dummy_schema.json\"), indent=2)\n",
    "inferred_schema.to_script(os.path.join(\"dummy_data\", \"inferred_dummy_schema.py\"))\n",
    "# TODO: open the schema (choose one of the two files and edit it)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you are done editing, you can read the schema again and validate your dataframe against it. From the python script you can directly import the schema."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "schema = pa.DataFrameSchema.from_json(os.path.join(\"dummy_data\", \"inferred_dummy_schema.json\"))\n",
    "# OR:\n",
    "# from inferred_dummy_schema import schema\n",
    "# schema = schema\n",
    "\n",
    "schema.validate(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving the schema as a file is useful if you want to include data validation in your data pipeline. It also makes versioning of your data schema easier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
